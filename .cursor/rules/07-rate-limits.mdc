---
globs: ["**/lib/**/*.ts", "**/utils/**/*.ts", "**/services/**/*.ts"]
---

# Rate Limiting & Retry Patterns

Critical patterns for working with external APIs, especially LLMs.

## Exponential Backoff

```typescript
// utils/retry.ts
interface RetryOptions {
  maxRetries: number;
  baseDelayMs: number;
  maxDelayMs: number;
  retryOn?: (error: unknown) => boolean;
}

export async function withRetry<T>(
  fn: () => Promise<T>,
  options: RetryOptions
): Promise<T> {
  const { maxRetries, baseDelayMs, maxDelayMs, retryOn } = options;
  
  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      const shouldRetry = retryOn?.(error) ?? isRetryable(error);
      
      if (!shouldRetry || attempt === maxRetries) {
        throw error;
      }
      
      const delay = Math.min(
        baseDelayMs * Math.pow(2, attempt) + Math.random() * 1000,
        maxDelayMs
      );
      
      logger.warn(`Retry attempt ${attempt + 1}/${maxRetries}`, { delay });
      await sleep(delay);
    }
  }
  
  throw new Error('Unreachable');
}

function isRetryable(error: unknown): boolean {
  if (error instanceof Error) {
    // Rate limits, timeouts, server errors
    return /429|503|504|timeout|ECONNRESET/i.test(error.message);
  }
  return false;
}
```

## Rate Limiter

```typescript
// utils/rate-limiter.ts
export class RateLimiter {
  private timestamps: number[] = [];
  
  constructor(
    private maxRequests: number,
    private windowMs: number
  ) {}
  
  async acquire(): Promise<void> {
    const now = Date.now();
    this.timestamps = this.timestamps.filter(t => t > now - this.windowMs);
    
    if (this.timestamps.length >= this.maxRequests) {
      const waitTime = this.timestamps[0] + this.windowMs - now;
      await sleep(waitTime);
      return this.acquire();
    }
    
    this.timestamps.push(now);
  }
}

// Usage
const llmRateLimiter = new RateLimiter(50, 60_000); // 50 req/min

async function callLLM(prompt: string) {
  await llmRateLimiter.acquire();
  return anthropic.messages.create({ ... });
}
```

## Queue Pattern for Batch Processing

```typescript
// utils/queue.ts
export class RequestQueue<T, R> {
  private queue: Array<{
    item: T;
    resolve: (value: R) => void;
    reject: (error: unknown) => void;
  }> = [];
  private processing = false;
  
  constructor(
    private processor: (item: T) => Promise<R>,
    private concurrency: number = 1,
    private delayMs: number = 100
  ) {}
  
  async add(item: T): Promise<R> {
    return new Promise((resolve, reject) => {
      this.queue.push({ item, resolve, reject });
      this.process();
    });
  }
  
  private async process() {
    if (this.processing) return;
    this.processing = true;
    
    while (this.queue.length > 0) {
      const batch = this.queue.splice(0, this.concurrency);
      
      await Promise.all(
        batch.map(async ({ item, resolve, reject }) => {
          try {
            const result = await this.processor(item);
            resolve(result);
          } catch (error) {
            reject(error);
          }
        })
      );
      
      if (this.queue.length > 0) {
        await sleep(this.delayMs);
      }
    }
    
    this.processing = false;
  }
}
```

## LLM-Specific Limits

| Provider | RPM | TPM | Notes |
|----------|-----|-----|-------|
| OpenAI (GPT-4) | 500 | 30K | Tier 1 |
| Anthropic (Claude) | 50 | 40K | Default |
| OpenAI Embeddings | 3000 | 1M | Higher |

## Best Practices

- Always implement retry with exponential backoff
- Add jitter to prevent thundering herd
- Log all rate limit hits for monitoring
- Use queues for batch processing
- Implement circuit breakers for failing services
- Cache responses where appropriate
